{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using paperXai to get your personal arXiv daily digest\n",
    "\n",
    "The goal of this notebook is to help you use this package and to understand the different components of the pipeline. Briefly, the pipeline can be decomposed into the following sections which will correspond to the sections of this notebook.\n",
    "\n",
    "- [Fetching the latest arXiv papers](#fetching-the-latest-arxiv-papers)\n",
    "- [Embedding predefined user questions and sections](#embedding-predefined-user-questions-and-sections)\n",
    "- [Semantic retrieval](#semantic-retrieval)\n",
    "- [Generating an automatic report](#generating-an-automatic-report)\n",
    "- [Sending the personalized newsletter](#sending-the-personalized-newsletter)\n",
    "\n",
    "For any comments, feel free to reach out directly (see [here](https://sebastianpartarrieu.github.io/)), or via an issue on the github repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# initial imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "import paperxai.constants as constants\n",
    "import paperxai.credentials as credentials\n",
    "from paperxai.llms import OpenAI\n",
    "from paperxai.report.retriever import ReportRetriever\n",
    "from paperxai.prompt.base import Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = credentials.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the latest arXiv papers\n",
    "\n",
    "See script `scripts/get_arxiv_papers.py` to fetch the documents using the arXiv API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ../scripts/get_arxiv_papers.py --max_results 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/base_papers.csv\")\n",
    "df_new_articles = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/current_papers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding predefined user questions and sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding articles\n",
    "\n",
    "Let's embed the different articles we've obtained. We pay little attention here to performance, if you want to run this on a larger dataset, it may be worth batching calls and saving the embeddings in a dedicated array (vector database is definitely not useful at this scale).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenAI(\n",
    "    chat_model=\"gpt-3.5-turbo\",\n",
    "    embedding_model=\"text-embedding-ada-002\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "df_new_articles[\"Embeddings\"] = df_new_articles[\"String_representation\"].apply(\n",
    "    lambda x: openai_model.get_embeddings(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new_articles.to_csv(\n",
    "#     constants.ROOT_DIR + \"/data/arxiv/current_papers_with_embeddings.csv\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings = np.vstack(df_new_articles[\"Embeddings\"].values)\n",
    "np.save(constants.ROOT_DIR + \"/data/arxiv/article_embeddings.npy\", article_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompt()\n",
    "report_retriever = ReportRetriever(\n",
    "    language_model=openai_model,\n",
    "    prompter=prompter,\n",
    "    papers_embedding=article_embeddings,\n",
    "    df_papers=df_new_articles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_retriever.get_chat_response_to_question(\"What are the latest developments around large language inference and quantization?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating an automatic report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending the personalized newsletter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
