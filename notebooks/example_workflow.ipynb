{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Using paperXai to get your personal arXiv daily digest\n",
            "\n",
            "The goal of this notebook is to help you use this package and to understand the different components of the pipeline. Briefly, the pipeline can be decomposed into the following sections which will correspond to the sections of this notebook.\n",
            "\n",
            "- [Fetching the latest arXiv papers](#fetching-the-latest-arxiv-papers)\n",
            "- [Embedding predefined user questions and sections](#embedding-predefined-user-questions-and-sections)\n",
            "- [Semantic retrieval](#semantic-retrieval--generating-an-automatic-report)\n",
            "- [Sending the personalized newsletter](#sending-the-personalized-newsletter)\n",
            "\n",
            "For any comments, feel free to reach out directly (see [here](https://sebastianpartarrieu.github.io/)), or via an issue on the github repository.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 69,
         "metadata": {},
         "outputs": [],
         "source": [
            "# initial imports\n",
            "%load_ext autoreload\n",
            "%autoreload 2\n",
            "\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import openai\n",
            "\n",
            "import paperxai.constants as constants\n",
            "import paperxai.credentials as credentials\n",
            "from paperxai.llms import OpenAI\n",
            "from paperxai.papers import Arxiv\n",
            "from paperxai.report.retriever import ReportRetriever\n",
            "from paperxai.prompt.base import Prompt\n",
            "from paperxai.loading import load_config"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "openai.api_key = credentials.OPENAI_API_KEY"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Fetching the latest arXiv papers\n",
            "\n",
            "See script `scripts/get_arxiv_papers.py` if you want to run this as a script. **Make sure** to tweak the `config.yml` file to change the questions according to what you want to learn/track in the latest papers.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "#!python ../scripts/get_arxiv_papers.py --max_results 1000"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "config = load_config(\"../config.yml\")\n",
            "arxiv = Arxiv()\n",
            "arxiv.get_papers(categories=config[\"arxiv-categories\"], max_results=1000)\n",
            "arxiv.write_papers()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_papers = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/current_papers.csv\",\n",
            "                        parse_dates=[\"Published Date\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Embedding predefined user questions and sections\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Embedding articles\n",
            "\n",
            "Let's embed the different articles we've obtained. We pay little attention here to performance, if you want to run this on a larger dataset, it may be worth batching calls and saving the embeddings in a dedicated array (vector database is definitely not useful at this scale).\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "metadata": {},
         "outputs": [
            {
               "ename": "ConnectionError",
               "evalue": "HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f74f4b14c70>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno -3] Temporary failure in name resolution)\"))",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    204\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    206\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    207\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39mraise\u001b[39;00m LocationParseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    953\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 954\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
                  "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
                  "\nThe above exception was the direct cause of the following exception:\n",
                  "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[39m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[39m.\u001b[39mproxy\u001b[39m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[39mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[39m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/connectionpool.py:1092\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1092\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[0;32m--> 611\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    612\u001b[0m server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m NameResolutionError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n",
                  "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f74f4b14c70>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno -3] Temporary failure in name resolution)",
                  "\nThe above exception was the direct cause of the following exception:\n",
                  "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
                  "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f74f4b14c70>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno -3] Temporary failure in name resolution)\"))",
                  "\nDuring handling of the above exception, another exception occurred:\n",
                  "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
                  "\u001b[1;32m/home/sebastian/Documents/MyCS/paperxai/notebooks/example_workflow.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sebastian/Documents/MyCS/paperxai/notebooks/example_workflow.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m openai_model \u001b[39m=\u001b[39m OpenAI(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sebastian/Documents/MyCS/paperxai/notebooks/example_workflow.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     chat_model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sebastian/Documents/MyCS/paperxai/notebooks/example_workflow.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     embedding_model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-embedding-ada-002\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sebastian/Documents/MyCS/paperxai/notebooks/example_workflow.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sebastian/Documents/MyCS/paperxai/notebooks/example_workflow.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sebastian/Documents/MyCS/paperxai/notebooks/example_workflow.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
                  "File \u001b[0;32m~/Documents/MyCS/paperxai/src/paperxai/llms/openai.py:20\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, chat_model, embedding_model, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_model \u001b[39m=\u001b[39m chat_model\n\u001b[1;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m=\u001b[39m embedding_model\n\u001b[0;32m---> 20\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(provider\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mopenai\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemperature \u001b[39m=\u001b[39m temperature\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens \u001b[39m=\u001b[39m max_tokens\n",
                  "File \u001b[0;32m~/Documents/MyCS/paperxai/src/paperxai/llms/base.py:9\u001b[0m, in \u001b[0;36mBaseLLM.__init__\u001b[0;34m(self, provider)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, provider: \u001b[39mstr\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovider \u001b[39m=\u001b[39m provider\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_tokenizer()\n\u001b[1;32m     10\u001b[0m     \u001b[39m# ensure that the tokenizer has an encode method\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, \u001b[39m\"\u001b[39m\u001b[39mencode\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mTokenizer must have an encode method\u001b[39m\u001b[39m\"\u001b[39m\n",
                  "File \u001b[0;32m~/Documents/MyCS/paperxai/src/paperxai/llms/openai.py:25\u001b[0m, in \u001b[0;36mOpenAI.set_tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_tokenizer\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tiktoken\u001b[39m.\u001b[39;49mencoding_for_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_model)\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/tiktoken/model.py:75\u001b[0m, in \u001b[0;36mencoding_for_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m encoding_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not automatically map \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m to a tokeniser. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease use `tiktok.get_encoding` to explicitly get the tokeniser you expect.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m get_encoding(encoding_name)\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/tiktoken/registry.py:63\u001b[0m, in \u001b[0;36mget_encoding\u001b[0;34m(encoding_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown encoding \u001b[39m\u001b[39m{\u001b[39;00mencoding_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m constructor \u001b[39m=\u001b[39m ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[0;32m---> 63\u001b[0m enc \u001b[39m=\u001b[39m Encoding(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconstructor())\n\u001b[1;32m     64\u001b[0m ENCODINGS[encoding_name] \u001b[39m=\u001b[39m enc\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m enc\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/tiktoken_ext/openai_public.py:64\u001b[0m, in \u001b[0;36mcl100k_base\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcl100k_base\u001b[39m():\n\u001b[0;32m---> 64\u001b[0m     mergeable_ranks \u001b[39m=\u001b[39m load_tiktoken_bpe(\n\u001b[1;32m     65\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mhttps://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m     special_tokens \u001b[39m=\u001b[39m {\n\u001b[1;32m     68\u001b[0m         ENDOFTEXT: \u001b[39m100257\u001b[39m,\n\u001b[1;32m     69\u001b[0m         FIM_PREFIX: \u001b[39m100258\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m         ENDOFPROMPT: \u001b[39m100276\u001b[39m,\n\u001b[1;32m     73\u001b[0m     }\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     75\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcl100k_base\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpat_str\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m(?i:\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms|\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt|\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre|\u001b[39m\u001b[39m'\u001b[39m\u001b[39mve|\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm|\u001b[39m\u001b[39m'\u001b[39m\u001b[39mll|\u001b[39m\u001b[39m'\u001b[39m\u001b[39md)|[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mp\u001b[39m\u001b[39m{L}\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39mp\u001b[39m\u001b[39m{N}\u001b[39;00m\u001b[39m]?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mp\u001b[39m\u001b[39m{L}\u001b[39;00m\u001b[39m+|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mp\u001b[39m\u001b[39m{N}\u001b[39;00m\u001b[39m{\u001b[39m\u001b[39m1,3}| ?[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mp\u001b[39m\u001b[39m{L}\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39mp\u001b[39m\u001b[39m{N}\u001b[39;00m\u001b[39m]+[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn]*|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn]+|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+(?!\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS)|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m\"\"\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmergeable_ranks\u001b[39m\u001b[39m\"\u001b[39m: mergeable_ranks,\n\u001b[1;32m     78\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecial_tokens\u001b[39m\u001b[39m\"\u001b[39m: special_tokens,\n\u001b[1;32m     79\u001b[0m     }\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/tiktoken/load.py:116\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mbytes\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# NB: do not add caching to this function\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     contents \u001b[39m=\u001b[39m read_file_cached(tiktoken_bpe_file)\n\u001b[1;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    118\u001b[0m         base64\u001b[39m.\u001b[39mb64decode(token): \u001b[39mint\u001b[39m(rank)\n\u001b[1;32m    119\u001b[0m         \u001b[39mfor\u001b[39;00m token, rank \u001b[39min\u001b[39;00m (line\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m contents\u001b[39m.\u001b[39msplitlines() \u001b[39mif\u001b[39;00m line)\n\u001b[1;32m    120\u001b[0m     }\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/tiktoken/load.py:48\u001b[0m, in \u001b[0;36mread_file_cached\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(cache_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     46\u001b[0m         \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> 48\u001b[0m contents \u001b[39m=\u001b[39m read_file(blobpath)\n\u001b[1;32m     50\u001b[0m os\u001b[39m.\u001b[39mmakedirs(cache_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m tmp_filename \u001b[39m=\u001b[39m cache_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid4()) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.tmp\u001b[39m\u001b[39m\"\u001b[39m\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/tiktoken/load.py:24\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m     23\u001b[0m \u001b[39m# avoiding blobfile for public files helps avoid auth issues, like MFA prompts\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(blobpath)\n\u001b[1;32m     25\u001b[0m resp\u001b[39m.\u001b[39mraise_for_status()\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m resp\u001b[39m.\u001b[39mcontent\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
                  "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.9/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
                  "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f74f4b14c70>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno -3] Temporary failure in name resolution)\"))"
               ]
            }
         ],
         "source": [
            "openai_model = OpenAI(\n",
            "    chat_model=\"gpt-3.5-turbo\",\n",
            "    embedding_model=\"text-embedding-ada-002\",\n",
            "    temperature=0.0,\n",
            "    max_tokens=1000,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 41,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_papers[\"Embeddings\"] = df_papers[\"String_representation\"].apply(\n",
            "    lambda x: openai_model.get_embeddings(text=x)\n",
            ")\n",
            "paper_embeddings = df_papers[\"Embeddings\"].values\n",
            "paper_embeddings = np.vstack(paper_embeddings)\n",
            "np.save(constants.ROOT_DIR + \"/data/arxiv/papers_embeddings.npy\", paper_embeddings)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 45,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(981, 1536)"
                  ]
               },
               "execution_count": 45,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "paper_embeddings.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "# df_paper = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/current_papers_with_embeddings.csv\")\n",
            "# article_embeddings = np.load(constants.ROOT_DIR + \"/data/arxiv/article_embeddings.npy\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Semantic retrieval & Generating an automatic report\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 47,
         "metadata": {},
         "outputs": [],
         "source": [
            "prompter = Prompt()\n",
            "report_retriever = ReportRetriever(\n",
            "    language_model=openai_model,\n",
            "    prompter=prompter,\n",
            "    papers_embedding=paper_embeddings,\n",
            "    df_papers=df_papers,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 51,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Getting responses for section: Large Language Model inference optimization\n",
                  "Answering question: What are the latest developments around large language inference and quantization?\n",
                  "Answering question: What are the latest developments around large language inference memory optimization?\n",
                  "Getting responses for section: Large Language Model training optimization\n",
                  "Answering question: What are the latest developments around large language models and distributed training across multiple GPUs?\n",
                  "Getting responses for section: Large Language Model and medicine\n",
                  "Answering question: What are the latest developments around large language models and medicine?\n",
                  "Section: Large Language Model inference optimization\n",
                  "\n",
                  "Question: What are the latest developments around large language inference and quantization?\n",
                  "LLM response: The latest developments around large language inference and quantization include exploring the feasibility of quantum natural language processing algorithms on noisy intermediate-scale quantum (NISQ) devices for language translation, harnessing quantum parallelism and entanglement to efficiently process linguistic data (Abbaszade, 2023), the emergence of high-level human abilities in large language models (LLMs) such as the ability to interpret novel metaphors (Ichien, 2023), and the development of a new type of model quantization approach called MRQ (model re-quantization) that supports multiple quantization schemes and quickly transforms existing quantized models to meet different quantization requirements (Manohara, 2023).\n",
                  "Question: What are the latest developments around large language inference memory optimization?\n",
                  "LLM response: The latest developments around large language inference memory optimization include the emergence of high-level human abilities in large language models (LLMs), such as the ability to interpret complex novel metaphors (Ichien, 2023), the use of the \"Skeleton-of-Thought\" (SoT) approach to decrease generation latency and improve answer quality (Ning, 2023), and the fine-tuning of LLMs to automatically translate natural language sentences into Description Logic for ontology enrichment (Mateiu, 2023).\n",
                  "\n",
                  "Section: Large Language Model training optimization\n",
                  "\n",
                  "Question: What are the latest developments around large language models and distributed training across multiple GPUs?\n",
                  "LLM response: The latest developments around large language models and distributed training across multiple GPUs include the emergent ability of GPT-4, a state-of-the-art large language model, to interpret complex novel metaphors, as demonstrated in a study by Ichien et al. (2023), the potential for large language models to revolutionize personalization systems by enabling active user engagement and expanding the scope of personalized services, as discussed by Chen et al. (2023), and the critical need for specialized training and model selection to address challenges related to bias and sensitivity in large language models, as highlighted by Hajikhani et al. (2023).\n",
                  "\n",
                  "Section: Large Language Model and medicine\n",
                  "\n",
                  "Question: What are the latest developments around large language models and medicine?\n",
                  "LLM response: The latest developments around large language models and medicine include the use of TrialGPT, a novel architecture employing large language models (LLMs) to predict criterion-level eligibility for clinical trials based on patient notes, as well as the development of local LLMs that can be fine-tuned for specific medical tasks such as extracting structured condition codes from pathology reports. These advancements demonstrate the potential of LLMs to assist in patient recruitment for clinical trials and perform complex medical tasks using accessible hardware (Jin, 2023; Bumgardner, 2023). Additionally, recent research has shown that LLMs, such as GPT-4, have acquired an emergent ability to interpret complex novel metaphors, indicating their potential for creative human abilities (Ichien, 2023).\n",
                  "\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "report = report_retriever.create_report()\n",
            "report_retriever.print_report()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 72,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "HTML is saved to /display/reports/2023-08-04-report.html, open it in your browser to view the report\n"
               ]
            }
         ],
         "source": [
            "report_retriever.write_report(format=\"html\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Sending the personalized newsletter\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "llms",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.16"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}