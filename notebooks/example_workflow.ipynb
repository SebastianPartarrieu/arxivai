{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using paperXai to get your personal arXiv daily digest\n",
    "\n",
    "The goal of this notebook is to help you use this package and to understand the different components of the pipeline. Briefly, the pipeline can be decomposed into the following sections which will correspond to the sections of this notebook.\n",
    "\n",
    "- [Fetching the latest arXiv papers](#fetching-the-latest-arxiv-papers)\n",
    "- [Embedding predefined user questions and sections](#embedding-predefined-user-questions-and-sections)\n",
    "- [Semantic retrieval](#semantic-retrieval--generating-an-automatic-report)\n",
    "- [Sending the personalized newsletter](#sending-the-personalized-newsletter)\n",
    "\n",
    "For any comments, feel free to reach out directly (see [here](https://sebastianpartarrieu.github.io/)), or via an issue on the github repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "import paperxai.constants as constants\n",
    "import paperxai.credentials as credentials\n",
    "from paperxai.llms import OpenAI\n",
    "from paperxai.papers import Arxiv\n",
    "from paperxai.report.retriever import ReportRetriever\n",
    "from paperxai.prompt.base import Prompt\n",
    "from paperxai.loading import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = credentials.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the latest arXiv papers\n",
    "\n",
    "See script `scripts/get_arxiv_papers.py` if you want to run this as a script. **Make sure** to tweak the `config.yml` file to change the questions according to what you want to learn/track in the latest papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ../scripts/get_arxiv_papers.py --max_results 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"../config.yml\")\n",
    "arxiv = Arxiv()\n",
    "arxiv.get_papers(categories=config[\"arxiv-categories\"], max_results=1000)\n",
    "arxiv.write_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/base_papers.csv\")\n",
    "df_new_articles = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/current_papers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding predefined user questions and sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding articles\n",
    "\n",
    "Let's embed the different articles we've obtained. We pay little attention here to performance, if you want to run this on a larger dataset, it may be worth batching calls and saving the embeddings in a dedicated array (vector database is definitely not useful at this scale).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenAI(\n",
    "    chat_model=\"gpt-3.5-turbo\",\n",
    "    embedding_model=\"text-embedding-ada-002\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_articles[\"Embeddings\"] = df_new_articles[\"String_representation\"].apply(\n",
    "    lambda x: openai_model.get_embeddings(x)\n",
    ")\n",
    "\n",
    "df_new_articles.to_csv(\n",
    "    constants.ROOT_DIR + \"/data/arxiv/current_papers_with_embeddings.csv\", index=False\n",
    ")\n",
    "\n",
    "article_embeddings = np.vstack(df_new_articles[\"Embeddings\"].values)\n",
    "np.save(constants.ROOT_DIR + \"/data/arxiv/article_embeddings.npy\", article_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_articles = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/current_papers_with_embeddings.csv\")\n",
    "article_embeddings = np.load(constants.ROOT_DIR + \"/data/arxiv/article_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic retrieval & Generating an automatic report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompt()\n",
    "report_retriever = ReportRetriever(\n",
    "    language_model=openai_model,\n",
    "    prompter=prompter,\n",
    "    papers_embedding=article_embeddings,\n",
    "    df_papers=df_new_articles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting responses for section: Large Language Model inference optimization\n",
      "Answering question: What are the latest developments around large language inference and quantization?\n",
      "Answering question: What are the latest developments around large language inference and Mixture of Experts?\n",
      "Answering question: What are the latest developments around large language inference memory optimization?\n",
      "Getting responses for section: Large Language Model training optimization\n",
      "Answering question: What are the latest developments around large language models and distributed training across multiple GPUs?\n",
      "Answering question: What are the latest developments around large language models and the tradeoff between model size and number of training tokens?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Large Language Model inference optimization': {'questions': ['What are the latest developments around large language inference and quantization?',\n",
       "   'What are the latest developments around large language inference and Mixture of Experts?',\n",
       "   'What are the latest developments around large language inference memory optimization?'],\n",
       "  'chat_responses': ['The latest developments around large language inference and quantization include the introduction of the QuIP method, which utilizes incoherence processing to improve quantization algorithms for large language models (Chee, 2023), the proposal of multilevel large language models that unify generic and specific models to improve performance based on user input and internet information (Gong, 2023), and the development of myQASR, a personalized mixed-precision quantization method for automatic speech recognition models that tailors quantization schemes for diverse users under any memory requirement (Fish, 2023).',\n",
       "   'The latest developments around large language inference and Mixture of Experts include the unification of generic and specific models into a larger map that can improve each other based on user input and information from the internet, the reduction of linguistic bias in multilingual language processing systems through technological design and methodology, and the use of large language models to assist in identifying suitable clinical trials for individual patients based on free-text patient notes (Gong, 2023; Bella, 2023; Jin, 2023).',\n",
       "   \"The latest developments around large language inference memory optimization include the unification of generic and specific models into a larger map, inspired by the functionality of the human brain, to achieve more complex high-level functionality, as well as the extension of large language models' capabilities by directly attaching a small audio encoder for speech recognition, allowing them to outperform monolingual baselines and perform multilingual speech recognition, and the use of large language models as competitive near cold-start recommenders for language- and item-based preferences, providing competitive recommendation performance for pure language-based preferences in comparison to item-based collaborative filtering methods. (Gong, 2023; Fathullah, 2023; Sanner, 2023)\"],\n",
       "  'papers': [                                                 Title  \\\n",
       "   485  QuIP: 2-Bit Quantization of Large Language Mod...   \n",
       "   507      Multilevel Large Language Models for Everyone   \n",
       "   644  A Model for Every User and Budget: Label-Free ...   \n",
       "   \n",
       "                                      URL  \\\n",
       "   485  http://arxiv.org/abs/2307.13304v1   \n",
       "   507  http://arxiv.org/abs/2307.13221v1   \n",
       "   644  http://arxiv.org/abs/2307.12659v1   \n",
       "   \n",
       "                                                 Abstract  \\\n",
       "   485  This work studies post-training parameter quan...   \n",
       "   507  Large language models have made significant pr...   \n",
       "   644  Recent advancement in Automatic Speech Recogni...   \n",
       "   \n",
       "                                                  Authors  \\\n",
       "   485  Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Ch...   \n",
       "   507                                       Yuanhao Gong   \n",
       "   644           Edward Fish, Umberto Michieli, Mete Ozay   \n",
       "   \n",
       "                   Published Date Category      Paper ID  \\\n",
       "   485  2023-07-25 07:44:06+00:00    cs.LG  2307.13304v1   \n",
       "   507  2023-07-25 03:18:04+00:00    cs.CV  2307.13221v1   \n",
       "   644  2023-07-24 10:03:28+00:00    cs.SD  2307.12659v1   \n",
       "   \n",
       "                                    String_representation  \\\n",
       "   485  Title: QuIP: 2-Bit Quantization of Large Langu...   \n",
       "   507  Title: Multilevel Large Language Models for Ev...   \n",
       "   644  Title: A Model for Every User and Budget: Labe...   \n",
       "   \n",
       "                                               Embeddings  \n",
       "   485  [-0.01728013  0.01658418  0.00675583 ... -0.01...  \n",
       "   507  [-0.01605953  0.00900636  0.02384515 ... -0.02...  \n",
       "   644  [-0.01319194  0.01290099  0.01115531 ... -0.01...  ,\n",
       "                                                    Title  \\\n",
       "   507      Multilevel Large Language Models for Everyone   \n",
       "   456       Towards Bridging the Digital Language Divide   \n",
       "   8    Matching Patients to Clinical Trials with Larg...   \n",
       "   \n",
       "                                      URL  \\\n",
       "   507  http://arxiv.org/abs/2307.13221v1   \n",
       "   456  http://arxiv.org/abs/2307.13405v1   \n",
       "   8    http://arxiv.org/abs/2307.15051v1   \n",
       "   \n",
       "                                                 Abstract  \\\n",
       "   507  Large language models have made significant pr...   \n",
       "   456  It is a well-known fact that current AI-based ...   \n",
       "   8    Clinical trials are vital in advancing drug de...   \n",
       "   \n",
       "                                                  Authors  \\\n",
       "   507                                       Yuanhao Gong   \n",
       "   456  Gábor Bella, Paula Helm, Gertraud Koch, Fausto...   \n",
       "   8    Qiao Jin, Zifeng Wang, Charalampos S. Floudas,...   \n",
       "   \n",
       "                   Published Date Category      Paper ID  \\\n",
       "   507  2023-07-25 03:18:04+00:00    cs.CV  2307.13221v1   \n",
       "   456  2023-07-25 10:53:20+00:00    cs.CL  2307.13405v1   \n",
       "   8    2023-07-27 17:56:56+00:00    cs.CL  2307.15051v1   \n",
       "   \n",
       "                                    String_representation  \\\n",
       "   507  Title: Multilevel Large Language Models for Ev...   \n",
       "   456  Title: Towards Bridging the Digital Language D...   \n",
       "   8    Title: Matching Patients to Clinical Trials wi...   \n",
       "   \n",
       "                                               Embeddings  \n",
       "   507  [-0.01605953  0.00900636  0.02384515 ... -0.02...  \n",
       "   456  [-0.01890001  0.00556485  0.01849033 ... -0.02...  \n",
       "   8    [-0.0036512   0.01217997  0.01594646 ... -0.01...  ,\n",
       "                                                    Title  \\\n",
       "   507      Multilevel Large Language Models for Everyone   \n",
       "   977  Prompting Large Language Models with Speech Re...   \n",
       "   217  Large Language Models are Competitive Near Col...   \n",
       "   \n",
       "                                      URL  \\\n",
       "   507  http://arxiv.org/abs/2307.13221v1   \n",
       "   977  http://arxiv.org/abs/2307.11795v1   \n",
       "   217  http://arxiv.org/abs/2307.14225v1   \n",
       "   \n",
       "                                                 Abstract  \\\n",
       "   507  Large language models have made significant pr...   \n",
       "   977  Large language models have proven themselves h...   \n",
       "   217  Traditional recommender systems leverage users...   \n",
       "   \n",
       "                                                  Authors  \\\n",
       "   507                                       Yuanhao Gong   \n",
       "   977  Yassir Fathullah, Chunyang Wu, Egor Lakomkin, ...   \n",
       "   217  Scott Sanner, Krisztian Balog, Filip Radlinski...   \n",
       "   \n",
       "                   Published Date Category      Paper ID  \\\n",
       "   507  2023-07-25 03:18:04+00:00    cs.CV  2307.13221v1   \n",
       "   977  2023-07-21 08:39:15+00:00  eess.AS  2307.11795v1   \n",
       "   217  2023-07-26 14:47:15+00:00    cs.IR  2307.14225v1   \n",
       "   \n",
       "                                    String_representation  \\\n",
       "   507  Title: Multilevel Large Language Models for Ev...   \n",
       "   977  Title: Prompting Large Language Models with Sp...   \n",
       "   217  Title: Large Language Models are Competitive N...   \n",
       "   \n",
       "                                               Embeddings  \n",
       "   507  [-0.01605953  0.00900636  0.02384515 ... -0.02...  \n",
       "   977  [-0.03564498  0.01179389  0.00079905 ... -0.01...  \n",
       "   217  [-0.02092572  0.00825622 -0.00199786 ...  0.00...  ]},\n",
       " 'Large Language Model training optimization': {'questions': ['What are the latest developments around large language models and distributed training across multiple GPUs?',\n",
       "   'What are the latest developments around large language models and the tradeoff between model size and number of training tokens?'],\n",
       "  'chat_responses': ['The latest developments around large language models and distributed training across multiple GPUs include the unification of generic and field-specific models into a larger map that can improve each other based on user input and information from the internet, inspired by the functionality of the human brain (Gong, 2023). Additionally, an optimized network architecture has been proposed that partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects, reducing network cost by up to 75% compared to state-of-the-art networks without compromising performance (Wang, 2023). Furthermore, a system has been developed that uses large language models to provide immediate feedback to students in flipped classroom preparation learning, addressing challenges in engagement and providing context-aligned answers to student questions (Uchiyama, 2023).',\n",
       "   'The latest developments around large language models and the tradeoff between model size and number of training tokens include the investigation of the utility of Large Language Models (LLMs) in tasks such as converting a text-based prompt into a design specification, transforming a design into manufacturing instructions, producing a design space and design variations, computing the performance of a design, and searching for designs predicated on performance, as well as the unification of generic and field-specific large language models into a larger map that can improve each other based on user personal input and information from the internet, and the evaluation of thirty-two LLMs in interpreting radiology reports, providing insights into their performance, strengths, and weaknesses within the medical domain (Makatura, 2023; Gong, 2023; Liu, 2023).'],\n",
       "  'papers': [                                                 Title  \\\n",
       "   507      Multilevel Large Language Models for Everyone   \n",
       "   788  Optimized Network Architectures for Large Lang...   \n",
       "   987  Large Language Model-based System to Provide I...   \n",
       "   \n",
       "                                      URL  \\\n",
       "   507  http://arxiv.org/abs/2307.13221v1   \n",
       "   788  http://arxiv.org/abs/2307.12169v1   \n",
       "   987  http://arxiv.org/abs/2307.11388v1   \n",
       "   \n",
       "                                                 Abstract  \\\n",
       "   507  Large language models have made significant pr...   \n",
       "   788  This paper challenges the well-established par...   \n",
       "   987  This paper proposes a system that uses large l...   \n",
       "   \n",
       "                                                  Authors  \\\n",
       "   507                                       Yuanhao Gong   \n",
       "   788  Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Y...   \n",
       "   987    Shintaro Uchiyama, Kyoji Umemura, Yusuke Morita   \n",
       "   \n",
       "                   Published Date Category      Paper ID  \\\n",
       "   507  2023-07-25 03:18:04+00:00    cs.CV  2307.13221v1   \n",
       "   788  2023-07-22 21:18:41+00:00    cs.NI  2307.12169v1   \n",
       "   987  2023-07-21 06:59:53+00:00    cs.HC  2307.11388v1   \n",
       "   \n",
       "                                    String_representation  \\\n",
       "   507  Title: Multilevel Large Language Models for Ev...   \n",
       "   788  Title: Optimized Network Architectures for Lar...   \n",
       "   987  Title: Large Language Model-based System to Pr...   \n",
       "   \n",
       "                                               Embeddings  \n",
       "   507  [-0.01605953  0.00900636  0.02384515 ... -0.02...  \n",
       "   788  [-0.03379066  0.01851177  0.01019061 ... -0.02...  \n",
       "   987  [-0.03003681  0.0187832   0.00074965 ... -0.01...  ,\n",
       "                                                    Title  \\\n",
       "   375  How Can Large Language Models Help Humans in D...   \n",
       "   507      Multilevel Large Language Models for Everyone   \n",
       "   370  Evaluating Large Language Models for Radiology...   \n",
       "   \n",
       "                                      URL  \\\n",
       "   375  http://arxiv.org/abs/2307.14377v1   \n",
       "   507  http://arxiv.org/abs/2307.13221v1   \n",
       "   370  http://arxiv.org/abs/2307.13693v2   \n",
       "   \n",
       "                                                 Abstract  \\\n",
       "   375  The advancement of Large Language Models (LLMs...   \n",
       "   507  Large language models have made significant pr...   \n",
       "   370  The rise of large language models (LLMs) has m...   \n",
       "   \n",
       "                                                  Authors  \\\n",
       "   375  Liane Makatura, Michael Foshey, Bohan Wang, Fe...   \n",
       "   507                                       Yuanhao Gong   \n",
       "   370  Zhengliang Liu, Tianyang Zhong, Yiwei Li, Yuto...   \n",
       "   \n",
       "                   Published Date Category      Paper ID  \\\n",
       "   375  2023-07-25 17:30:38+00:00    cs.CL  2307.14377v1   \n",
       "   507  2023-07-25 03:18:04+00:00    cs.CV  2307.13221v1   \n",
       "   370  2023-07-25 17:57:18+00:00    cs.CL  2307.13693v2   \n",
       "   \n",
       "                                    String_representation  \\\n",
       "   375  Title: How Can Large Language Models Help Huma...   \n",
       "   507  Title: Multilevel Large Language Models for Ev...   \n",
       "   370  Title: Evaluating Large Language Models for Ra...   \n",
       "   \n",
       "                                               Embeddings  \n",
       "   375  [-0.00569948  0.01606435  0.00213344 ...  0.00...  \n",
       "   507  [-0.01605953  0.00900636  0.02384515 ... -0.02...  \n",
       "   370  [-0.01048704  0.03709306  0.02674473 ... -0.00...  ]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_retriever.create_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: Large Language Model inference optimization\n",
      "\n",
      "Question: What are the latest developments around large language inference and quantization?\n",
      "LLM response: The latest developments around large language inference and quantization include the introduction of the QuIP method, which utilizes incoherence processing to improve quantization algorithms for large language models (Chee, 2023), the proposal of multilevel large language models that unify generic and specific models to improve performance based on user input and internet information (Gong, 2023), and the development of myQASR, a personalized mixed-precision quantization method for automatic speech recognition models that tailors quantization schemes for diverse users under any memory requirement (Fish, 2023).\n",
      "Question: What are the latest developments around large language inference and Mixture of Experts?\n",
      "LLM response: The latest developments around large language inference and Mixture of Experts include the unification of generic and specific models into a larger map that can improve each other based on user input and information from the internet, the reduction of linguistic bias in multilingual language processing systems through technological design and methodology, and the use of large language models to assist in identifying suitable clinical trials for individual patients based on free-text patient notes (Gong, 2023; Bella, 2023; Jin, 2023).\n",
      "Question: What are the latest developments around large language inference memory optimization?\n",
      "LLM response: The latest developments around large language inference memory optimization include the unification of generic and specific models into a larger map, inspired by the functionality of the human brain, to achieve more complex high-level functionality, as well as the extension of large language models' capabilities by directly attaching a small audio encoder for speech recognition, allowing them to outperform monolingual baselines and perform multilingual speech recognition, and the use of large language models as competitive near cold-start recommenders for language- and item-based preferences, providing competitive recommendation performance for pure language-based preferences in comparison to item-based collaborative filtering methods. (Gong, 2023; Fathullah, 2023; Sanner, 2023)\n",
      "\n",
      "Section: Large Language Model training optimization\n",
      "\n",
      "Question: What are the latest developments around large language models and distributed training across multiple GPUs?\n",
      "LLM response: The latest developments around large language models and distributed training across multiple GPUs include the unification of generic and field-specific models into a larger map that can improve each other based on user input and information from the internet, inspired by the functionality of the human brain (Gong, 2023). Additionally, an optimized network architecture has been proposed that partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects, reducing network cost by up to 75% compared to state-of-the-art networks without compromising performance (Wang, 2023). Furthermore, a system has been developed that uses large language models to provide immediate feedback to students in flipped classroom preparation learning, addressing challenges in engagement and providing context-aligned answers to student questions (Uchiyama, 2023).\n",
      "Question: What are the latest developments around large language models and the tradeoff between model size and number of training tokens?\n",
      "LLM response: The latest developments around large language models and the tradeoff between model size and number of training tokens include the investigation of the utility of Large Language Models (LLMs) in tasks such as converting a text-based prompt into a design specification, transforming a design into manufacturing instructions, producing a design space and design variations, computing the performance of a design, and searching for designs predicated on performance, as well as the unification of generic and field-specific large language models into a larger map that can improve each other based on user personal input and information from the internet, and the evaluation of thirty-two LLMs in interpreting radiology reports, providing insights into their performance, strengths, and weaknesses within the medical domain (Makatura, 2023; Gong, 2023; Liu, 2023).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_retriever.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HTML newsletter template to format the report with (i) the sections, questions and answers and (ii) all the papers retrieved for each question\n",
    "HTML_newsletter_template = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "body {\n",
    "    font-family: Arial, Helvetica, sans-serif;\n",
    "    font-size: 14px;\n",
    "    line-height: 1.5;\n",
    "    color: #333333;\n",
    "    background-color: #ffffff;\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "    -webkit-text-size-adjust: none;\n",
    "    -ms-text-size-adjust: none;\n",
    "}\n",
    "table {\n",
    "    border-collapse: collapse;\n",
    "    border-spacing: 0;\n",
    "}\n",
    "td {\n",
    "    padding: 0;\n",
    "}\n",
    "img {\n",
    "    border: 0;\n",
    "    -ms-interpolation-mode: bicubic;\n",
    "}\n",
    "a {\n",
    "    color: #ee6a56;\n",
    "    text-decoration: underline;\n",
    "}\n",
    "h1 {\n",
    "    font-family: Arial, Helvetica, sans-serif;\n",
    "    font-size: 28px;\n",
    "    line-height: 1.2;\n",
    "    color: #333333;\n",
    "    font-weight: bold;\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0;\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\">\n",
    "    <tr>\n",
    "        <td align=\"center\" bgcolor=\"#ffffff\" style=\"padding: 40px 0 30px 0;\">\n",
    "            <img src=\"https://images.unsplash.com/photo-1580584126903-c17d41830450?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1639&q=80\" width=\"300\" height=\"230\" style=\"display: block;\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td bgcolor=\"#ffffff\" style=\"padding: 40px 30px 40px 30px;\">\n",
    "            <table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\">\n",
    "                <tr>\n",
    "                    <td style=\"padding: 20px 0 30px 0;\">\n",
    "                        <h1>PaperXAI Report</h1>\n",
    "                    </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"padding: 0 0 20px 0;\">\n",
    "                        <h2>Introduction</h2>\n",
    "                        <p>Hi there,</p>\n",
    "                        <p>Here is your PaperXAI report. We hope you enjoy it!</p>\n",
    "                    </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"padding: 0 0 20px 0;\">\n",
    "                        <h2>Report</h2>\n",
    "                        <p>{report_string}</p>\n",
    "                    </td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## format the report using the report template\n",
    "\n",
    "def format_save_report(report_string: str, HTML_template: str) -> None:\n",
    "    \"\"\"\n",
    "    Format the report using the report template\n",
    "    \"\"\"\n",
    "    report_HTML = HTML_template.replace(\"{report_string}\", report_string)\n",
    "    #save HTML to file\n",
    "    with open(\"../display/report.html\", \"w\") as f:\n",
    "        f.write(report_HTML)\n",
    "    print(\"HTML is saved to display/report.html, open it in your browser to view the report\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<html>\\n<head>\\n<style>\\nbody {\\n    font-family: Arial, Helvetica, sans-serif;\\n    font-size: 14px;\\n    line-height: 1.5;\\n    color: #333333;\\n    background-color: #ffffff;\\n    margin: 0;\\n    padding: 0;\\n    -webkit-text-size-adjust: none;\\n    -ms-text-size-adjust: none;\\n}\\ntable {\\n    border-collapse: collapse;\\n    border-spacing: 0;\\n}\\ntd {\\n    padding: 0;\\n}\\nimg {\\n    border: 0;\\n    -ms-interpolation-mode: bicubic;\\n}\\na {\\n    color: #ee6a56;\\n    text-decoration: underline;\\n}\\nh1 {\\n    font-family: Arial, Helvetica, sans-serif;\\n    font-size: 28px;\\n    line-height: 1.2;\\n    color: #333333;\\n    font-weight: bold;\\n    margin-top: 0;\\n    margin-bottom: 0;\\n    text-align: center;\\n}\\n</style>\\n</head>\\n<body>\\n<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\">\\n    <tr>\\n        <td align=\"center\" bgcolor=\"#ffffff\" style=\"padding: 40px 0 30px 0;\">\\n            <img src=\"https://images.unsplash.com/photo-1580584126903-c17d41830450?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1639&q=80\" width=\"300\" height=\"230\" style=\"display: block;\" />\\n        </td>\\n    </tr>\\n    <tr>\\n        <td bgcolor=\"#ffffff\" style=\"padding: 40px 30px 40px 30px;\">\\n            <table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\">\\n                <tr>\\n                    <td style=\"padding: 20px 0 30px 0;\">\\n                        <h1>PaperXAI Report</h1>\\n                    </td>\\n                </tr>\\n                <tr>\\n                    <td style=\"padding: 0 0 20px 0;\">\\n                        <h2>Introduction</h2>\\n                        <p>Hi there,</p>\\n                        <p>Here is your PaperXAI report. We hope you enjoy it!</p>\\n                    </td>\\n                </tr>\\n                <tr>\\n                    <td style=\"padding: 0 0 20px 0;\">\\n                        <h2>Report</h2>\\n                        <p><h2> Section: Large Language Model inference optimization</h2><h3> Question: What are the latest developments around large language inference and quantization?</h3><p> LLM response: The latest developments around large language inference and quantization include the introduction of the QuIP method, which utilizes incoherence processing to improve quantization algorithms for large language models (Chee, 2023), the proposal of multilevel large language models that unify generic and specific models to improve performance based on user input and internet information (Gong, 2023), and the development of myQASR, a personalized mixed-precision quantization method for automatic speech recognition models that tailors quantization schemes for diverse users under any memory requirement (Fish, 2023).</p><h4> Papers </h4><ul><li> QuIP: 2-Bit Quantization of Large Language Models With Guarantees. Chee et al. 2023-07-25</li><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> A Model for Every User and Budget: Label-Free and Personalized\\n  Mixed-Precision Quantization. Fish et al. 2023-07-24</li></ul><h3> Question: What are the latest developments around large language inference and Mixture of Experts?</h3><p> LLM response: The latest developments around large language inference and Mixture of Experts include the unification of generic and specific models into a larger map that can improve each other based on user input and information from the internet, the reduction of linguistic bias in multilingual language processing systems through technological design and methodology, and the use of large language models to assist in identifying suitable clinical trials for individual patients based on free-text patient notes (Gong, 2023; Bella, 2023; Jin, 2023).</p><h4> Papers </h4><ul><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> Towards Bridging the Digital Language Divide. Bella et al. 2023-07-25</li><li> Matching Patients to Clinical Trials with Large Language Models. Jin et al. 2023-07-27</li></ul><h3> Question: What are the latest developments around large language inference memory optimization?</h3><p> LLM response: The latest developments around large language inference memory optimization include the unification of generic and specific models into a larger map, inspired by the functionality of the human brain, to achieve more complex high-level functionality, as well as the extension of large language models\\' capabilities by directly attaching a small audio encoder for speech recognition, allowing them to outperform monolingual baselines and perform multilingual speech recognition, and the use of large language models as competitive near cold-start recommenders for language- and item-based preferences, providing competitive recommendation performance for pure language-based preferences in comparison to item-based collaborative filtering methods. (Gong, 2023; Fathullah, 2023; Sanner, 2023)</p><h4> Papers </h4><ul><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> Prompting Large Language Models with Speech Recognition Abilities. Fathullah et al. 2023-07-21</li><li> Large Language Models are Competitive Near Cold-start Recommenders for\\n  Language- and Item-based Preferences. Sanner et al. 2023-07-26</li></ul><h2> Section: Large Language Model training optimization</h2><h3> Question: What are the latest developments around large language models and distributed training across multiple GPUs?</h3><p> LLM response: The latest developments around large language models and distributed training across multiple GPUs include the unification of generic and field-specific models into a larger map that can improve each other based on user input and information from the internet, inspired by the functionality of the human brain (Gong, 2023). Additionally, an optimized network architecture has been proposed that partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects, reducing network cost by up to 75% compared to state-of-the-art networks without compromising performance (Wang, 2023). Furthermore, a system has been developed that uses large language models to provide immediate feedback to students in flipped classroom preparation learning, addressing challenges in engagement and providing context-aligned answers to student questions (Uchiyama, 2023).</p><h4> Papers </h4><ul><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> Optimized Network Architectures for Large Language Model Training with\\n  Billions of Parameters. Wang et al. 2023-07-22</li><li> Large Language Model-based System to Provide Immediate Feedback to\\n  Students in Flipped Classroom Preparation Learning. Uchiyama et al. 2023-07-21</li></ul><h3> Question: What are the latest developments around large language models and the tradeoff between model size and number of training tokens?</h3><p> LLM response: The latest developments around large language models and the tradeoff between model size and number of training tokens include the investigation of the utility of Large Language Models (LLMs) in tasks such as converting a text-based prompt into a design specification, transforming a design into manufacturing instructions, producing a design space and design variations, computing the performance of a design, and searching for designs predicated on performance, as well as the unification of generic and field-specific large language models into a larger map that can improve each other based on user personal input and information from the internet, and the evaluation of thirty-two LLMs in interpreting radiology reports, providing insights into their performance, strengths, and weaknesses within the medical domain (Makatura, 2023; Gong, 2023; Liu, 2023).</p><h4> Papers </h4><ul><li> How Can Large Language Models Help Humans in Design and Manufacturing?. Makatura et al. 2023-07-25</li><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> Evaluating Large Language Models for Radiology Natural Language\\n  Processing. Liu et al. 2023-07-25</li></ul></p>\\n                    </td>\\n                </tr>\\n            </table>\\n        </td>\\n    </tr>\\n</table>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_save_report(report_retriever.format_report(), HTML_newsletter_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<h2> Section: Large Language Model inference optimization</h2><h3> Question: What are the latest developments around large language inference and quantization?</h3><p> LLM response: The latest developments around large language inference and quantization include the introduction of the QuIP method, which utilizes incoherence processing to improve quantization algorithms for large language models (Chee, 2023), the proposal of multilevel large language models that unify generic and specific models to improve performance based on user input and internet information (Gong, 2023), and the development of myQASR, a personalized mixed-precision quantization method for automatic speech recognition models that tailors quantization schemes for diverse users under any memory requirement (Fish, 2023).</p><h4> Papers </h4><ul><li> QuIP: 2-Bit Quantization of Large Language Models With Guarantees. Chee et al. 2023-07-25</li><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> A Model for Every User and Budget: Label-Free and Personalized\\n  Mixed-Precision Quantization. Fish et al. 2023-07-24</li></ul><h3> Question: What are the latest developments around large language inference and Mixture of Experts?</h3><p> LLM response: The latest developments around large language inference and Mixture of Experts include the unification of generic and specific models into a larger map that can improve each other based on user input and information from the internet, the reduction of linguistic bias in multilingual language processing systems through technological design and methodology, and the use of large language models to assist in identifying suitable clinical trials for individual patients based on free-text patient notes (Gong, 2023; Bella, 2023; Jin, 2023).</p><h4> Papers </h4><ul><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> Towards Bridging the Digital Language Divide. Bella et al. 2023-07-25</li><li> Matching Patients to Clinical Trials with Large Language Models. Jin et al. 2023-07-27</li></ul><h3> Question: What are the latest developments around large language inference memory optimization?</h3><p> LLM response: The latest developments around large language inference memory optimization include the unification of generic and specific models into a larger map, inspired by the functionality of the human brain, to achieve more complex high-level functionality, as well as the extension of large language models' capabilities by directly attaching a small audio encoder for speech recognition, allowing them to outperform monolingual baselines and perform multilingual speech recognition, and the use of large language models as competitive near cold-start recommenders for language- and item-based preferences, providing competitive recommendation performance for pure language-based preferences in comparison to item-based collaborative filtering methods. (Gong, 2023; Fathullah, 2023; Sanner, 2023)</p><h4> Papers </h4><ul><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> Prompting Large Language Models with Speech Recognition Abilities. Fathullah et al. 2023-07-21</li><li> Large Language Models are Competitive Near Cold-start Recommenders for\\n  Language- and Item-based Preferences. Sanner et al. 2023-07-26</li></ul><h2> Section: Large Language Model training optimization</h2><h3> Question: What are the latest developments around large language models and distributed training across multiple GPUs?</h3><p> LLM response: The latest developments around large language models and distributed training across multiple GPUs include the unification of generic and field-specific models into a larger map that can improve each other based on user input and information from the internet, inspired by the functionality of the human brain (Gong, 2023). Additionally, an optimized network architecture has been proposed that partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects, reducing network cost by up to 75% compared to state-of-the-art networks without compromising performance (Wang, 2023). Furthermore, a system has been developed that uses large language models to provide immediate feedback to students in flipped classroom preparation learning, addressing challenges in engagement and providing context-aligned answers to student questions (Uchiyama, 2023).</p><h4> Papers </h4><ul><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> Optimized Network Architectures for Large Language Model Training with\\n  Billions of Parameters. Wang et al. 2023-07-22</li><li> Large Language Model-based System to Provide Immediate Feedback to\\n  Students in Flipped Classroom Preparation Learning. Uchiyama et al. 2023-07-21</li></ul><h3> Question: What are the latest developments around large language models and the tradeoff between model size and number of training tokens?</h3><p> LLM response: The latest developments around large language models and the tradeoff between model size and number of training tokens include the investigation of the utility of Large Language Models (LLMs) in tasks such as converting a text-based prompt into a design specification, transforming a design into manufacturing instructions, producing a design space and design variations, computing the performance of a design, and searching for designs predicated on performance, as well as the unification of generic and field-specific large language models into a larger map that can improve each other based on user personal input and information from the internet, and the evaluation of thirty-two LLMs in interpreting radiology reports, providing insights into their performance, strengths, and weaknesses within the medical domain (Makatura, 2023; Gong, 2023; Liu, 2023).</p><h4> Papers </h4><ul><li> How Can Large Language Models Help Humans in Design and Manufacturing?. Makatura et al. 2023-07-25</li><li> Multilevel Large Language Models for Everyone. Gong et al. 2023-07-25</li><li> Evaluating Large Language Models for Radiology Natural Language\\n  Processing. Liu et al. 2023-07-25</li></ul>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_retriever.format_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending the personalized newsletter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
