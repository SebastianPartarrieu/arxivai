{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using paperXai to get your personal arXiv daily digest\n",
    "\n",
    "The goal of this notebook is to help you use this package and to understand the different components of the pipeline. Briefly, the pipeline can be decomposed into the following sections which will correspond to the sections of this notebook.\n",
    "\n",
    "- [Fetching the latest arXiv papers](#fetching-the-latest-arxiv-papers)\n",
    "- [Embedding predefined user questions and sections](#embedding-predefined-user-questions-and-sections)\n",
    "- [Semantic retrieval](#semantic-retrieval--generating-an-automatic-report)\n",
    "- [Sending the personalized newsletter](#sending-the-personalized-newsletter)\n",
    "\n",
    "For any comments, feel free to reach out directly (see [here](https://sebastianpartarrieu.github.io/)), or via an issue on the github repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# initial imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "import paperxai.constants as constants\n",
    "import paperxai.credentials as credentials\n",
    "from paperxai.llms import OpenAI\n",
    "from paperxai.papers import Arxiv\n",
    "from paperxai.report.retriever import ReportRetriever\n",
    "from paperxai.prompt.base import Prompt\n",
    "from paperxai.loading import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = credentials.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the latest arXiv papers\n",
    "\n",
    "See script `scripts/get_arxiv_papers.py` if you want to run this as a script. **Make sure** to tweak the `config.yml` file to change the questions according to what you want to learn/track in the latest papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ../scripts/get_arxiv_papers.py --max_results 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"../config.yml\")\n",
    "arxiv = Arxiv()\n",
    "arxiv.get_papers(categories=config[\"arxiv-categories\"], max_results=1000)\n",
    "arxiv.write_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/current_papers.csv\",\n",
    "                        parse_dates=[\"Published Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding predefined user questions and sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding articles\n",
    "\n",
    "Let's embed the different articles we've obtained. We pay little attention here to performance, if you want to run this on a larger dataset, it may be worth batching calls and saving the embeddings in a dedicated array (vector database is definitely not useful at this scale).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenAI(\n",
    "    chat_model=\"gpt-3.5-turbo\",\n",
    "    embedding_model=\"text-embedding-ada-002\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers[\"Embeddings\"] = df_papers[\"String_representation\"].apply(\n",
    "    lambda x: openai_model.get_embeddings(text=x)\n",
    ")\n",
    "paper_embeddings = df_papers[\"Embeddings\"].values\n",
    "paper_embeddings = np.vstack(paper_embeddings)\n",
    "np.save(constants.ROOT_DIR + \"/data/arxiv/papers_embeddings.npy\", paper_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(981, 1536)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_paper = pd.read_csv(constants.ROOT_DIR + \"/data/arxiv/current_papers_with_embeddings.csv\")\n",
    "# article_embeddings = np.load(constants.ROOT_DIR + \"/data/arxiv/article_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic retrieval & Generating an automatic report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompt()\n",
    "report_retriever = ReportRetriever(\n",
    "    language_model=openai_model,\n",
    "    prompter=prompter,\n",
    "    papers_embedding=paper_embeddings,\n",
    "    df_papers=df_papers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting responses for section: Large Language Model inference optimization\n",
      "Answering question: What are the latest developments around large language inference and quantization?\n",
      "Answering question: What are the latest developments around large language inference memory optimization?\n",
      "Getting responses for section: Large Language Model training optimization\n",
      "Answering question: What are the latest developments around large language models and distributed training across multiple GPUs?\n",
      "Getting responses for section: Large Language Model and medicine\n",
      "Answering question: What are the latest developments around large language models and medicine?\n",
      "Section: Large Language Model inference optimization\n",
      "\n",
      "Question: What are the latest developments around large language inference and quantization?\n",
      "LLM response: The latest developments around large language inference and quantization include exploring the feasibility of quantum natural language processing algorithms on noisy intermediate-scale quantum (NISQ) devices for language translation, harnessing quantum parallelism and entanglement to efficiently process linguistic data (Abbaszade, 2023), the emergence of high-level human abilities in large language models (LLMs) such as the ability to interpret novel metaphors (Ichien, 2023), and the development of a new type of model quantization approach called MRQ (model re-quantization) that supports multiple quantization schemes and quickly transforms existing quantized models to meet different quantization requirements (Manohara, 2023).\n",
      "Question: What are the latest developments around large language inference memory optimization?\n",
      "LLM response: The latest developments around large language inference memory optimization include the emergence of high-level human abilities in large language models (LLMs), such as the ability to interpret complex novel metaphors (Ichien, 2023), the use of the \"Skeleton-of-Thought\" (SoT) approach to decrease generation latency and improve answer quality (Ning, 2023), and the fine-tuning of LLMs to automatically translate natural language sentences into Description Logic for ontology enrichment (Mateiu, 2023).\n",
      "\n",
      "Section: Large Language Model training optimization\n",
      "\n",
      "Question: What are the latest developments around large language models and distributed training across multiple GPUs?\n",
      "LLM response: The latest developments around large language models and distributed training across multiple GPUs include the emergent ability of GPT-4, a state-of-the-art large language model, to interpret complex novel metaphors, as demonstrated in a study by Ichien et al. (2023), the potential for large language models to revolutionize personalization systems by enabling active user engagement and expanding the scope of personalized services, as discussed by Chen et al. (2023), and the critical need for specialized training and model selection to address challenges related to bias and sensitivity in large language models, as highlighted by Hajikhani et al. (2023).\n",
      "\n",
      "Section: Large Language Model and medicine\n",
      "\n",
      "Question: What are the latest developments around large language models and medicine?\n",
      "LLM response: The latest developments around large language models and medicine include the use of TrialGPT, a novel architecture employing large language models (LLMs) to predict criterion-level eligibility for clinical trials based on patient notes, as well as the development of local LLMs that can be fine-tuned for specific medical tasks such as extracting structured condition codes from pathology reports. These advancements demonstrate the potential of LLMs to assist in patient recruitment for clinical trials and perform complex medical tasks using accessible hardware (Jin, 2023; Bumgardner, 2023). Additionally, recent research has shown that LLMs, such as GPT-4, have acquired an emergent ability to interpret complex novel metaphors, indicating their potential for creative human abilities (Ichien, 2023).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = report_retriever.create_report()\n",
    "report_retriever.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML is saved to /display/reports/2023-08-04-report.html, open it in your browser to view the report\n"
     ]
    }
   ],
   "source": [
    "report_retriever.write_report(format=\"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending the personalized newsletter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
